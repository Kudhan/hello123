#T-SNE

from sklearn.datasets import load_digits
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np

# Load Digits dataset
digits = load_digits()
X = digits.data
y = digits.target  # True labels (for visualization, not used in clustering)

# Step 1: Perform t-SNE for dimensionality reduction
tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)
X_tsne = tsne.fit_transform(X)

# Step 2: Apply K-Means clustering
kmeans = KMeans(n_clusters=10, random_state=42)
clusters = kmeans.fit_predict(X_tsne)

# Step 3: Visualize the clusters
plt.figure(figsize=(10, 7))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=clusters, cmap='tab10', s=50, alpha=0.7)
plt.colorbar(scatter, ticks=range(10), label="Cluster ID")
plt.title("K-Means Clustering on t-SNE-Reduced Digits Data", fontsize=14)
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.show()


#T-SNE

from sklearn.datasets import load_digits
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np

# Load Digits dataset
digits = load_digits()
X = digits.data
y = digits.target  # True labels (for visualization, not used in clustering)

# Step 1: Perform t-SNE for dimensionality reduction
tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)
X_tsne = tsne.fit_transform(X)

# Step 2: Apply K-Means clustering
kmeans = KMeans(n_clusters=10, random_state=42)
clusters = kmeans.fit_predict(X_tsne)

# Step 3: Visualize the clusters
plt.figure(figsize=(10, 7))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=clusters, cmap='tab10', s=50, alpha=0.7)
plt.colorbar(scatter, ticks=range(10), label="Cluster ID")
plt.title("K-Means Clustering on t-SNE-Reduced Digits Data", fontsize=14)
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.show()


#Code for Decision Tree and Model Evaluation in Python

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load a sample dataset similar to weather.numeric
data = {
    'Temperature': [85, 80, 83, 70, 68, 65, 75, 72, 69, 81],
    'Humidity': [85, 90, 86, 96, 80, 70, 80, 95, 70, 75],
    'Windy': [0, 1, 0, 0, 0, 1, 1, 0, 1, 1],
    'PlayTennis': [0, 0, 1, 1, 1, 1, 1, 0, 1, 0],
}
df = pd.DataFrame(data)

# Split dataset into features and target
X = df.drop('PlayTennis', axis=1)
y = df['PlayTennis']

# Step 1: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 2: Train a Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Step 3: Make predictions on the test set
y_pred = clf.predict(X_test)

# Step 4: Evaluate the model's performance
print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Step 5: Visualize the Decision Tree
plt.figure(figsize=(12, 8))
plot_tree(clf, filled=True, feature_names=X.columns, class_names=['No', 'Yes'], rounded=True)
plt.title("Decision Tree Classifier - PlayTennis Prediction")
plt.show()



#Na誰ve Bayes Classifier on Weather Numeric Dataset

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pandas as pd

# Sample numeric dataset for weather prediction (numeric features)
data_numeric = {
    'Temperature': [85, 80, 83, 70, 68, 65, 75, 72, 69, 81],
    'Humidity': [85, 90, 86, 96, 80, 70, 80, 95, 70, 75],
    'Windy': [0, 1, 0, 0, 0, 1, 1, 0, 1, 1],
    'PlayTennis': [0, 0, 1, 1, 1, 1, 1, 0, 1, 0]
}
df_numeric = pd.DataFrame(data_numeric)

# Split dataset into features and target
X_numeric = df_numeric.drop('PlayTennis', axis=1)
y_numeric = df_numeric['PlayTennis']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_numeric, y_numeric, test_size=0.3, random_state=42)

# Train Na誰ve Bayes Classifier (GaussianNB for numeric data)
nb_model_numeric = GaussianNB()
nb_model_numeric.fit(X_train, y_train)

# Predictions
y_pred_numeric = nb_model_numeric.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred_numeric))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_numeric))
print("Classification Report:")
print(classification_report(y_test, y_pred_numeric))



# Na誰ve Bayes Classifier on Weather Nominal Dataset
from sklearn.naive_bayes import CategoricalNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pandas as pd

# Sample nominal dataset for weather prediction (categorical features)
data_nominal = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Mild', 'Mild', 'Cool', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'High'],
    'Windy': ['False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True'],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No']
}

df_nominal = pd.DataFrame(data_nominal)

# Convert categorical features to numeric codes
df_nominal_encoded = df_nominal.apply(lambda x: pd.factorize(x)[0])

# Split dataset into features and target
X_nominal = df_nominal_encoded.drop('PlayTennis', axis=1)
y_nominal = df_nominal_encoded['PlayTennis']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_nominal, y_nominal, test_size=0.3, random_state=42)

# Train Na誰ve Bayes Classifier (CategoricalNB for nominal data)
nb_model_nominal = CategoricalNB()
nb_model_nominal.fit(X_train, y_train)

# Predictions
y_pred_nominal = nb_model_nominal.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred_nominal))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_nominal))
print("Classification Report:")
print(classification_report(y_test, y_pred_nominal))



#Exploratory Data Analysis (EDA)

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset (use any dataset, here using the famous Titanic dataset)
url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
df = pd.read_csv(url)

# Basic Information
print(df.info())  # Data types and missing values
print(df.describe())  # Summary statistics

# Visualizing Missing Data
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Data Visualization')
plt.show()

# Distribution of Age (with missing values handled)
plt.figure(figsize=(10, 6))
sns.histplot(df['Age'].dropna(), kde=True, bins=30)  # Dropping missing values for Age
plt.title('Age Distribution')
plt.show()

# Correlation Matrix (handle non-numeric columns by selecting only numeric ones)
plt.figure(figsize=(10, 6))
sns.heatmap(df.select_dtypes(include=[np.number]).corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

# Pairplot of selected features (ensure 'Survived' exists, and handle missing values)
sns.pairplot(df[['Age', 'Fare', 'SibSp', 'Parch', 'Survived']].dropna(), hue='Survived')
plt.show()



# Import required libraries
from mlxtend.frequent_patterns import apriori, association_rules
import pandas as pd

# Sample data: list of transactions where each transaction is a list of items
dataset = [
    ['Milk', 'Bread', 'Butter'],
    ['Bread', 'Butter', 'Jam'],
    ['Milk', 'Bread', 'Butter', 'Jam'],
    ['Bread', 'Butter'],
    ['Milk', 'Bread', 'Butter'],
    ['Milk', 'Bread']
]

# Convert the list of transactions into a one-hot encoded DataFrame
df = pd.DataFrame(columns=['Milk', 'Bread', 'Butter', 'Jam'],
                  data=[[1, 1, 1, 0], [0, 1, 1, 1], [1, 1, 1, 1],
                        [0, 1, 1, 0], [1, 1, 1, 0], [1, 1, 0, 0]])

# Apply the Apriori algorithm to find frequent itemsets
frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)

# Display the frequent itemsets
print("Frequent Itemsets:")
print(frequent_itemsets)

# Generate the association rules from the frequent itemsets
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1,num_itemsets=0)

# Display the generated association rules
print("\nAssociation Rules:")
print(rules)




#t-SNE vs PCA with K-Means
import numpy as np
import pandas as pd
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load the digits dataset
digits = load_digits()
X = digits.data
y = digits.target

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 1. Apply PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 2. Apply t-SNE
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X_scaled)

# 3. K-Means Clustering on PCA and t-SNE reduced data
kmeans_pca = KMeans(n_clusters=10, random_state=42)
kmeans_tsne = KMeans(n_clusters=10, random_state=42)

# Fit K-Means on PCA and t-SNE results
y_pca = kmeans_pca.fit_predict(X_pca)
y_tsne = kmeans_tsne.fit_predict(X_tsne)

# Plot the results
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# PCA Results
axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_pca, cmap='viridis')
axes[0].set_title("K-Means Clustering on PCA")
axes[0].set_xlabel("PC1")
axes[0].set_ylabel("PC2")

# t-SNE Results
axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_tsne, cmap='viridis')
axes[1].set_title("K-Means Clustering on t-SNE")
axes[1].set_xlabel("t-SNE1")
axes[1].set_ylabel("t-SNE2")

plt.tight_layout()
plt.show()




